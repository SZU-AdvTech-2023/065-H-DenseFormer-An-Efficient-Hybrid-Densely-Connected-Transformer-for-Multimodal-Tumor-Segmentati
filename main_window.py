# -*- coding: utf-8 -*-

# Form implementation generated from reading ui file 'd:\计算机前沿技术成果复现计划模版\H-DenseFormer-main\1.ui'
#
# Created by: PyQt5 UI code generator 5.15.7
#
# WARNING: Any manual changes made to this file will be lost when pyuic5 is
# run again.  Do not edit this file unless you know what you are doing.
import torch
import sys
from PyQt5 import QtCore, QtWidgets
from PyQt5.QtWidgets import QMainWindow,QFileDialog
import matplotlib.pyplot as plt
from models.HDenseFormer import HDenseFormer_16
from models.HDenseFormer_2D import HDenseFormer_2D_32
from torchvision import transforms
from data_utils.data_loader import DataGenerator, CropResize, To_Tensor, PETandCTNormalize, MRNormalize,Trunc_and_Normalize
from utils import dfs_remove_weight, hdf5_reader
import numpy as np
from torch.cuda.amp import autocast
from torch.nn import functional as F
import os

def cal_steps(image_size):
        patch_size = (144,144,144)
        step_size = (72,72,72)

        steps = []

        for dim in range(len(image_size)):
            if image_size[dim] <= patch_size[dim]:
                steps_here = [
                    0,
                ]
            else:
                max_step_value = image_size[dim] - patch_size[dim]
                num_steps = int(np.ceil((max_step_value) / step_size[dim])) + 1
                actual_step_size = max_step_value / (num_steps - 1)
                steps_here = [
                    int(np.round(actual_step_size * i))
                    for i in range(num_steps)
                ]

            steps.append(steps_here)

        return steps

class Ui_Form(object):
    def setupUi(self, Form):
        Form.setObjectName("Form")
        Form.resize(400, 300)
        self.pushButton = QtWidgets.QPushButton(Form)
        self.pushButton.setGeometry(QtCore.QRect(160, 110, 75, 23))
        self.pushButton.setObjectName("pushButton")
        self.pushButton_2 = QtWidgets.QPushButton(Form)
        self.pushButton_2.setGeometry(QtCore.QRect(160, 160, 75, 23))
        self.pushButton_2.setObjectName("pushButton_2")
        self.pushButton.clicked.connect(self.inference_2d)
        self.pushButton_2.clicked.connect(self.inference_3d)

        self.retranslateUi(Form)
        QtCore.QMetaObject.connectSlotsByName(Form)

    def retranslateUi(self, Form):
        _translate = QtCore.QCoreApplication.translate
        Form.setWindowTitle(_translate("Form", "Form"))
        self.pushButton.setText(_translate("Form", "2d分割"))
        self.pushButton_2.setText(_translate("Form", "3d分割"))

    def inference_2d(self):
        path=QFileDialog.getOpenFileName(directory=r'E:\PI-CAI22\test_2d_seg')[0] #获取绝对路径
        seri=path.split('/')[-1][:-5]
        print(seri)
        net=HDenseFormer_2D_32(in_channels=3, n_cls=2, image_size=(384,384), transformer_depth=24)
        net.load_state_dict(torch.load(r'ckpt\PI-CAI22\val_run_dice=0.54921.pth')['state_dict'],strict=False)
        net = net.cuda()
        net.eval()

        test_transformer = transforms.Compose([
            MRNormalize(),  #1
            To_Tensor(num_class=2)  #10
        ])

        with torch.no_grad():
            image = hdf5_reader(path, 'ct')
            label = hdf5_reader(path, 'seg').astype(int)
            sample = {'image': image, 'label': label}
            tmp=image.transpose((1,2,0)).copy()
            # Transform
            if test_transformer is not None:
                sample = test_transformer(sample)

            image = np.expand_dims(image, axis=0)

            data = torch.from_numpy(image)
            data = data.cuda()

            with autocast(False):
                predicted_patch = net(data)
                if isinstance(predicted_patch, tuple):
                    predicted_patch = predicted_patch[0]

            if isinstance(predicted_patch, list):
                predicted_patch = predicted_patch[0]

            output=torch.argmax(torch.softmax(predicted_patch, dim=1), 1).squeeze().cpu().numpy().astype(int)
            # measure run dice
            plt.figure('预处理后的图像')
            plt.imshow(tmp.astype(int))
            plt.figure('预测')
            plt.imshow(output,cmap='gray')
            plt.figure('真值')
            plt.imshow(label,cmap='gray')
            plt.show()
            torch.cuda.empty_cache()

            pre_path='segout/2d/pre/{}.jpg'.format(seri)
            if not os.path.exists(pre_path):
                plt.imsave(pre_path.format(seri), output,cmap='gray')
            label_path='segout/2d/label/{}.jpg'.format(seri)
            if not os.path.exists(label_path):
                plt.imsave(label_path.format(seri), label,cmap='gray')

    def inference_3d(self):
        path=QFileDialog.getOpenFileName(directory=r'E:\Hecktor21\test_3d_seg')[0] #获取绝对路径
        seri=path.split('/')[-1][:-5]
        print(seri)
        net=HDenseFormer_16(in_channels=2, n_cls=2,image_size=(144,144,144),transformer_depth=24)
        net.load_state_dict(torch.load(r'ckpt\Hecktor21\val_run_dice=0.79963.pth')['state_dict'],strict=False)
        net = net.cuda()
        net.eval()

        test_transformer = transforms.Compose([
            PETandCTNormalize(),  #2
            To_Tensor(num_class=2)  #6
        ])

        patch_size = (144,144,144)

        with torch.no_grad():
            image = hdf5_reader(path, 'ct')
            label = hdf5_reader(path, 'seg')
            sample = {'image': image, 'label': label}

            # Transform
            if test_transformer is not None:
                sample = test_transformer(sample)

            ori_image = np.asarray(sample['image'])

            new_image = np.expand_dims(ori_image, axis=0)

            aggregated_results = torch.zeros(
                [1, 2] +
                list(new_image.shape[2:]), ).cuda()
            aggregated_nb_of_predictions = torch.zeros(
                [1, 2] +
                list(new_image.shape[2:]), ).cuda()

            steps = cal_steps(ori_image.shape[1:])

            for x in steps[0]:
                lb_x = x
                ub_x = x + patch_size[0] if x + patch_size[
                    0] <= ori_image.shape[1] else ori_image.shape[1]
                for y in steps[1]:
                    lb_y = y
                    ub_y = y + patch_size[1] if y + patch_size[
                        1] <= ori_image.shape[2] else ori_image.shape[2]
                    for z in steps[2]:
                        lb_z = z
                        ub_z = z + patch_size[2] if z + patch_size[
                            2] <= ori_image.shape[3] else ori_image.shape[3]

                        image = ori_image[:, lb_x:ub_x, lb_y:ub_y,
                                            lb_z:ub_z]

                        image = np.expand_dims(image, axis=0)

                        data = torch.from_numpy(image).float()
                        data = data.cuda()

                        with autocast(False):
                            predicted_patch = net(data)
                            if isinstance(predicted_patch, tuple):
                                predicted_patch = predicted_patch[0]

                        if isinstance(predicted_patch, list):
                            predicted_patch = predicted_patch[0]

                        predicted_patch = F.softmax(predicted_patch, dim=1)
                        predicted_patch = F.interpolate(
                            predicted_patch,
                            (ub_x - lb_x, ub_y - lb_y, ub_z - lb_z))

                        aggregated_results[:, :, lb_x:ub_x, lb_y:ub_y,
                                            lb_z:
                                            ub_z] += predicted_patch  #* gaussian_importance_map
                        aggregated_nb_of_predictions[:, :, lb_x:ub_x,
                                                        lb_y:ub_y,
                                                        lb_z:ub_z] += 1
                        # aggregated_nb_of_predictions[:, :, lb_x:ub_x, lb_y:ub_y, lb_z:ub_z] += gaussian_importance_map

            output = aggregated_results / aggregated_nb_of_predictions

            # measure run dice
            output = torch.argmax(torch.softmax(
                output, dim=1), 1).detach().cpu().numpy().squeeze()  #N*H*W
            index=np.argmax(np.sum(label,(1,2))) #最大肿瘤区域的索引
            label=label[index].astype(int)
            output=output[index].astype(int)
            plt.figure('真值')
            plt.imshow(label,cmap='gray')
            plt.figure('预测')
            plt.imshow(output,cmap='gray')
            plt.show()
            torch.cuda.empty_cache()

            pre_path='segout/3d/pre/{}.jpg'.format(seri)
            if not os.path.exists(pre_path):
                plt.imsave(pre_path.format(seri), output,cmap='gray')
            label_path='segout/3d/label/{}.jpg'.format(seri)
            if not os.path.exists(label_path):
                plt.imsave(label_path.format(seri), label,cmap='gray')
            

app = QtWidgets.QApplication(sys.argv)
MainWindow = QMainWindow()
ui = Ui_Form()
ui.setupUi(MainWindow)
MainWindow.show()
sys.exit(app.exec_())